{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from get_data import get_yield\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Yield data\n",
    "df = get_yield(term=1)\n",
    "# to weekly and take diff\n",
    "df = df.resample(\"W-FRI\").last()\n",
    "df = df.diff().dropna()\n",
    "# Skip last two years of data\n",
    "df = df.truncate(after = pd.to_datetime('2023-2-10'))\n",
    "\n",
    "# corr = df_1.corr()\n",
    "\n",
    "Y = df.copy()\n",
    "Y = Y.sort_index()\n",
    "Y = Y.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Granger causlity test (just for UK-CAN for now)\n",
    "# Might need to make dict of dicts for all of the combinations\n",
    "gc_test_result = grangercausalitytests(Y[['CAN', 'UK']], maxlag=5, addconst=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAR\n",
    "model = VAR(Y)\n",
    "order = model.select_order(maxlags=7)\n",
    "print(order.summary())\n",
    "p = order.selected_orders[\"aic\"]\n",
    "results = model.fit(p)\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Granger Causality of everything\n",
    "\n",
    "# Statistical meaning, things with a low enough p value improve the prediction of of Y inside the VAR, eg: lags of CAN help\n",
    "# predict the UK change in yield\n",
    "\n",
    "for caused in results.names:\n",
    "    for causing in results.names:\n",
    "        if caused != causing:\n",
    "            test = results.test_causality(caused, [causing], kind='f')\n",
    "            print(f\"{causing} -> {caused}: p-value = {test.pvalue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fevd = results.fevd(10)\n",
    "fevd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_fevd(var_results, H=10, normalize=True):\n",
    "    \"\"\"\n",
    "    Generalized FEVD (Pesaran-Shin) for a fitted statsmodels VARResults.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    var_results : statsmodels.tsa.vector_ar.var_model.VARResults\n",
    "        Fitted VAR results (e.g., `results = model.fit(p)`).\n",
    "    H : int\n",
    "        Forecast horizon in steps (e.g., weeks). Uses horizons 0..H-1 in sums.\n",
    "    normalize : bool\n",
    "        If True, row-normalize so contributions sum to 1 for each equation at each horizon.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    fevd : np.ndarray\n",
    "        Array of shape (H, n, n) where fevd[h, i, j] is contribution of shock j\n",
    "        to variable i at horizon h (h=0..H-1). If normalize=True, each row sums to 1.\n",
    "    names : list\n",
    "        Variable names in order.\n",
    "    \"\"\"\n",
    "    names = list(var_results.names)\n",
    "    n = len(names)\n",
    "\n",
    "    # Moving-average (MA) representation coefficients Psi_k\n",
    "    # Psi has shape (H, n, n) with Psi[0] = I\n",
    "    Psi = var_results.ma_rep(H)\n",
    "\n",
    "    # Residual covariance matrix Σ (n x n)\n",
    "    Sigma = np.asarray(var_results.sigma_u)\n",
    "\n",
    "    # Precompute denominators: denom[h, i] = sum_{k=0}^{h} e_i' Psi_k Σ Psi_k' e_i\n",
    "    # We'll build horizon-by-horizon contributions using cumulative sums.\n",
    "    fevd = np.zeros((H, n, n), dtype=float)\n",
    "\n",
    "    # For each horizon h, compute cumulative sums from k=0..h\n",
    "    for h in range(H):\n",
    "        denom = np.zeros(n, dtype=float)\n",
    "        numer = np.zeros((n, n), dtype=float)\n",
    "\n",
    "        for k in range(h + 1):\n",
    "            A = Psi[k] @ Sigma  # (n x n)\n",
    "            # denom_i adds (Psi_k Σ Psi_k')_ii\n",
    "            denom += np.diag(A @ Psi[k].T)\n",
    "\n",
    "            # numer_{i,j} adds (e_i' Psi_k Σ e_j)^2 / σ_jj\n",
    "            # e_i' Psi_k Σ e_j is just A[i, j]\n",
    "            numer += (A ** 2)\n",
    "\n",
    "        # divide each column j by σ_jj (generalized shock scaling)\n",
    "        sigma_diag = np.diag(Sigma).copy()\n",
    "        # avoid division by 0 if any diag is 0 (shouldn't happen in sane VAR)\n",
    "        sigma_diag[sigma_diag == 0] = np.nan\n",
    "\n",
    "        numer = numer / sigma_diag  # broadcasts across rows\n",
    "\n",
    "        # contribution at horizon h\n",
    "        # θ_ij(h) = numer_ij / denom_i\n",
    "        # (broadcast denom_i across columns)\n",
    "        fevd[h] = numer / denom[:, None]\n",
    "\n",
    "        if normalize:\n",
    "            row_sums = fevd[h].sum(axis=1, keepdims=True)\n",
    "            fevd[h] = fevd[h] / row_sums\n",
    "\n",
    "    return fevd, names\n",
    "\n",
    "\n",
    "def fevd_table(fevd, names, var, horizons=None):\n",
    "    \"\"\"\n",
    "    Convenience: return a DataFrame for one dependent variable across horizons.\n",
    "    var : str, dependent variable name.\n",
    "    horizons : iterable of int or None -> use all.\n",
    "    \"\"\"\n",
    "    idx = names.index(var)\n",
    "    H = fevd.shape[0]\n",
    "    if horizons is None:\n",
    "        horizons = range(H)\n",
    "\n",
    "    rows = []\n",
    "    for h in horizons:\n",
    "        rows.append(fevd[h, idx, :])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=names, index=list(horizons))\n",
    "    df.index.name = \"horizon\"\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a table which shows how much of the yields is caused locally, versus impacted from others (I think)\n",
    "gfevd, names = generalized_fevd(results, H=10, normalize=True)\n",
    "uk_g = fevd_table(gfevd, names, \"UK\")\n",
    "can_g = fevd_table(gfevd, names, \"CAN\")\n",
    "us_g = fevd_table(gfevd, names, \"US\")\n",
    "\n",
    "uk_g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the Diebold-Yilmaz spillover table which is found in their papers\n",
    "\n",
    "theta = gfevd[9]\n",
    "N = theta.shape[0]\n",
    "\n",
    "total_spillover = (theta.sum() - np.trace(theta)) / N * 100\n",
    "\n",
    "to_others = theta.sum(axis=0) - np.diag(theta)\n",
    "from_others = theta.sum(axis=1) - np.diag(theta)\n",
    "net = to_others - from_others\n",
    "\n",
    "spill = pd.DataFrame({\n",
    "    \"TO_others\": to_others,\n",
    "    \"FROM_others\": from_others,\n",
    "    \"NET\": net\n",
    "}, index=names).sort_values(\"NET\", ascending=False)\n",
    "\n",
    "total_spillover, spill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from get_data import get_yield\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# ----------------------------\n",
    "# Data\n",
    "# ----------------------------\n",
    "df = get_yield(term=1)\n",
    "df = df.resample(\"W-FRI\").last()\n",
    "df = df.diff().dropna()\n",
    "df = df.truncate(after=pd.to_datetime(\"2023-02-10\"))\n",
    "df = df.sort_index()\n",
    "df = df.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "\n",
    "target = \"UK\"\n",
    "p = 5\n",
    "k_vol = 0.5\n",
    "vol_lookback = 52\n",
    "tcost = 0.0\n",
    "vol_target = 0.0\n",
    "\n",
    "idx = df.index\n",
    "y_uk = df[target].copy()\n",
    "rolling_vol = y_uk.rolling(vol_lookback).std()\n",
    "\n",
    "pred = pd.Series(index=idx, dtype=float)\n",
    "pos = pd.Series(index=idx, dtype=float)\n",
    "\n",
    "# ----------------------------\n",
    "# 70/30 split\n",
    "# ----------------------------\n",
    "n = len(df)\n",
    "split_i = int(np.floor(0.70 * n))\n",
    "split_date = idx[split_i]\n",
    "\n",
    "# Start of test loop must also respect p + vol lookback\n",
    "start_i = max(split_i, p + 1, vol_lookback + 1)\n",
    "\n",
    "# ----------------------------\n",
    "# Walk-forward ONLY on test period\n",
    "# ----------------------------\n",
    "for i in range(start_i, len(idx) - 1):\n",
    "    end_date = idx[i]\n",
    "\n",
    "    # expanding train set: everything up to end_date\n",
    "    train = df.loc[:end_date].copy()\n",
    "\n",
    "    model = VAR(train)\n",
    "    res = model.fit(p)\n",
    "\n",
    "    last_lags = train.values[-p:]\n",
    "    fcast = res.forecast(y=last_lags, steps=1)[0]\n",
    "    uk_j = res.names.index(target)\n",
    "\n",
    "    pred.loc[end_date] = fcast[uk_j]\n",
    "\n",
    "    vol = rolling_vol.loc[end_date]\n",
    "\n",
    "    if pd.isna(vol) or vol == 0.0:\n",
    "        pos.loc[end_date] = 0.0\n",
    "    else:\n",
    "        if abs(pred.loc[end_date]) < k_vol * vol:\n",
    "            pos.loc[end_date] = 0.0\n",
    "        else:\n",
    "            pos.loc[end_date] = -np.sign(pred.loc[end_date])\n",
    "\n",
    "    if vol_target > 0:\n",
    "        if (not pd.isna(vol)) and vol > 0:\n",
    "            pos.loc[end_date] *= (vol_target / vol)\n",
    "        else:\n",
    "            pos.loc[end_date] = 0.0\n",
    "\n",
    "pos = pos.fillna(0.0)\n",
    "\n",
    "# PnL\n",
    "pnl = pos.shift(1) * (-y_uk)\n",
    "turnover = (pos - pos.shift(1)).abs().fillna(0.0)\n",
    "pnl = pnl - tcost * turnover\n",
    "\n",
    "# Evaluate ONLY on test window\n",
    "test_mask = idx >= split_date\n",
    "pnl_bt = pnl.loc[test_mask].copy()\n",
    "\n",
    "cum = pnl_bt.cumsum()\n",
    "mean = pnl_bt.mean()\n",
    "std = pnl_bt.std(ddof=0)\n",
    "sharpe = np.nan\n",
    "if std and std > 0:\n",
    "    sharpe = (mean / std) * np.sqrt(52)\n",
    "\n",
    "trades = int((turnover.loc[pnl_bt.index] > 0).sum())\n",
    "pct_in_market = (pos.loc[pnl_bt.index] != 0).mean()\n",
    "\n",
    "print(\"Split date (70/30):\", split_date)\n",
    "print(\"Test window:\", pnl_bt.index.min(), \"->\", pnl_bt.index.max())\n",
    "print(\"Mean weekly PnL (per unit duration):\", mean)\n",
    "print(\"Std weekly PnL:\", std)\n",
    "print(\"Ann. Sharpe (52w):\", sharpe)\n",
    "print(\"Total cumulative PnL (per unit duration):\", cum.iloc[-1])\n",
    "print(\"Trades:\", trades)\n",
    "print(\"Pct weeks in market:\", pct_in_market)\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    \"UK_dy\": y_uk,\n",
    "    \"UK_pred_next\": pred,\n",
    "    \"UK_vol\": rolling_vol,\n",
    "    \"pos\": pos,\n",
    "    \"pnl\": pnl\n",
    "})\n",
    "print(out.loc[test_mask].tail(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
